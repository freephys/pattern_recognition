{
 "metadata": {
  "name": "",
  "signature": "sha256:0e860ce83620ea4095ea18ad2bddf45545657027d5aa1c9f1ed24cbc7b597546"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b> Pattern Classification \n",
      "\n",
      "V. Mirjalili \n",
      "04-04-2014 </b>\n",
      "\n",
      "<h1>Bayes Classification</h1>\n",
      "\n",
      "<h3>Description of input data </h3>\n",
      "\n",
      "Input data contains 2 features $\\bar{x} =\\left\\{x_1, x_2\\right\\}$. Each sample belongs to one of the three classes: $\\left\\{w_1, w_2, w_3\\right\\}$.\n",
      "Overall, there are 1500 samples. \n",
      "First we will read the data into a numpy 2D array"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "data = np.loadtxt('input_data.txt')\n",
      "print(data.shape)\n",
      "ntot = len(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1500, 3)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4> Constructing Training and test sets</h4>\n",
      "\n",
      "Next, we create a training subset that contains 70% of all data (randomly selected), \n",
      "and assign the rest of data to a test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#train_inx = (np.random.sample(100)*100).astype(int)\n",
      "train_index = np.random.choice(np.arange(ntot), size=0.7*ntot, replace=False)\n",
      "print(len(train_index))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1050\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = data[train_index,0:]\n",
      "#print(X_train[0:6])\n",
      "X_test = data[list(set(np.arange(ntot)) - set(train_index)),:]\n",
      "print(len(X_train), len(X_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1050, 450)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Bayes Decision Rule Given Class Conditional Densities (PDFs)</h3>\n",
      "\n",
      "If we assume that the class conditional densities are as below:\n",
      "\n",
      "$ p(\\underline{x} | w_1) \\sim N([0,0]^t, 4I) $\n",
      "\n",
      "$ p(\\bar{x} | w_2) \\sim N([10,0]^t, 4I) $\n",
      "\n",
      "$ p(\\bar{x} | w_3) \\sim N([5,5]^t, 5I) $\n",
      "\n",
      "Having equal priors, $ P(w_1) = P(w_2) = P(w_3) $, and Bayes formula for posterior probability $ P(w_i | \\bar{x}) = \\frac{p(\\bar{x}|w_i) P(w_i)}{P(\\bar{x})} $, then we set up the discriminant functions $g_i = \\ln P(w_i | \\bar{x})$ in which equal priors and other constant terms can be omitted to reach:\n",
      "\n",
      "$ g_i = -\\frac{1}{2}(x-\\mu_i)^t \\Sigma_i^{-1} (x-\\mu_i) -\\frac{1}{2} |\\Sigma_i| \\Longrightarrow -(x-\\mu_i)^t \\Sigma_i^{-1} (x-\\mu_i) - |\\Sigma_i|$\n",
      "\n",
      "\n",
      "Therefore, each pattern $x$ is classified to class $w_i$ whose discriminant function $g_i$ is maximum.\n",
      "\n",
      "Note that if we muliply it by $-1$, then we should consider minimum $g_i$ instead."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mahalanobis_sqdist(x, mean, Sig_inv):\n",
      "    '''\n",
      "    Calculates the Mahalanobis Distance of pattern x\n",
      "    to mean, given the inverse covariance matrix Sig_inv\n",
      "    '''\n",
      "    xdiff = x - mean\n",
      "    return(np.dot(np.dot(xdiff, Sig_inv),xdiff))\n",
      "\n",
      "#testing the function\n",
      "x=X_train[1,0:2]\n",
      "mahalanobis_sqdist(x, np.array([0,0]), np.array([[1,2], [3,4]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "54.045437769999992"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "def g_discriminant(x, mean, Sig_inv):\n",
      "    g = -mahalanobis_sqdist(x, mean, Sig_inv) + math.log(np.linalg.norm(Sig_inv))\n",
      "    return g\n",
      "\n",
      "#testing the function\n",
      "print g_discriminant(x, np.array([0,0]), np.array([[1,2], [3,4]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-52.3448390792\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we apply 3 dicriminant functions on each pattern $x$ in the test set, and assign $x$ to the class that maximizae $g_i(x)$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(X, m1,m2,m3, S1,S2,S3):\n",
      "    g1 = np.apply_along_axis(g_discriminant, 1, X, m1, S1)\n",
      "    g2 = np.apply_along_axis(g_discriminant, 1, X, m2, S2)\n",
      "    g3 = np.apply_along_axis(g_discriminant, 1, X, m3, S3)\n",
      "    all_g = np.vstack((g1, g2, g3)).T\n",
      "    print (all_g.shape)\n",
      "    #print all_g[0:6,:]\n",
      "    return (all_g.argmax(axis=1)+1)\n",
      "    \n",
      "ypred = predict(X_test[:,0:2], np.array([0,0]),np.array([10,0]),np.array([5,5]),  np.array([[4,0], [0,4]]),np.array([[4,0], [0,4]]),np.array([[5,0], [0,5]]))\n",
      "\n",
      "print float(sum(ypred!=X_test[:,2]))/len(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(450, 3)\n",
        "0.0711111111111\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3> Parameter Estimation: Maximum Likelihood Approach</h3>\n",
      "\n",
      "Now, we assume that we do <b>NOT</b> know the class conditional densities, but we know that they have Gaussian form. Therefore,\n",
      "\n",
      "$p(x | w_i) \\sim N(\\mu_i, \\Sigma_i) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_i|^{1/2}} \\exp(-\\frac{1}{2}(x-\\mu_i)^t \\Sigma_i^{-1} (x-\\mu_i))$\n",
      "\n",
      "And our goal is to estimate $\\mu_i$ and $\\Sigma_i$. We treat each class seprately, so for now we drop the class index $i$. The likelihood of observing a set $D=\\{x_1, x_2, ... x_n\\}$ is\n",
      "${\\displaystyle Likelihood = \\prod_{k=1}^n p(x_k) = \\prod_{k=1}^n \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp(-\\frac{1}{2}(x-\\mu)^t \\Sigma^{-1} (x-\\mu))}$\n",
      "\n",
      "Then, taking the $\\ln$ of both sides:\n",
      "${\\displaystyle Log Likelihood=L=\\sum_{k=1}^n -\\frac{1}{2}(x_k-\\mu)^t\\Sigma^{-1}(x_k-\\mu) - \\frac{1}{2}\\ln|\\Sigma| -\\frac{d}{2}\\ln(2\\pi)}$. For MLE, we have to find parameters $\\theta$ that maximizes $L\\Longrightarrow$, so we have to solve $\\frac{\\partial L}{\\partial \\theta} = 0$ where $\\theta$ can be $\\mu$ and $\\Sigma$\n",
      "\n",
      "${\\displaystyle \\frac{\\partial L}{\\partial \\mu} = \\sum_{k=1}^n \\frac{\\partial}{\\partial \\mu}[-\\frac{1}{2}(x_k-\\mu)^t \\Sigma^{-1} (x_k-\\mu)] = 0}$\n",
      "\n",
      "${\\displaystyle \\sum_{k=1}^n -\\frac{1}{2} (-2\\Sigma^{-1}(x_k-\\mu))= \\sum_{k=1}^n \\Sigma^{-1} x_k - \\sum_{k=1}^n \\Sigma^{-1}\\mu = 0}$\n",
      "\n",
      "${\\displaystyle \\Sigma^{-1} (\\sum_{k=1}^n x_k- \\sum_{k=1}^n \\mu) = 0 \\Longrightarrow \\sum_{k=1}^n x_k=\\sum_{k=1}^n \\mu = n \\mu}$ (Since $\\Sigma^{-1}$ cannot be zero). \n",
      "\n",
      "Then, ${\\displaystyle \\mu = \\frac{1}{n}\\sum_{k=1}^n x_k }$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b> MLE to compute $\\Sigma$</b>\n",
      "\n",
      "Next, we have to compute elements of matrix $\\Sigma$. so we have $L_k=(x_k-\\mu)^t \\Sigma^{-1} (x_k-\\mu) + \\ln|\\Sigma|$\n",
      "\n",
      "The easiest way is to compute elements of $\\Sigma^{-1}$ instead. So, we assume $\\Sigma^{-1}=\\begin{bmatrix} a & b \\\\ c &d \\\\  \\end{bmatrix}$, then the other term will also be simplified as $\\ln|\\Sigma|=\\ln\\frac{1}{|\\Sigma^{-1}|}=-\\ln|\\Sigma^{-1}|=-\\ln(ad-bc)$\n",
      "\n",
      "Then, we write our $LogLikelihood$ function as follows, (Note that $x_k=\\begin{bmatrix}x_k^1 \\\\ x_k^2 \\end{bmatrix}, \\text{ and } \\mu=\\begin{bmatrix} \\mu^1 \\\\ \\mu^2 \\end{bmatrix}$)\n",
      "\n",
      "$L_k = \\begin{bmatrix}x_k^1-\\mu^1 & x_k^2-\\mu^2\\end{bmatrix} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x_k^1-\\mu^1 \\\\ x_k^2-\\mu^2 \\end{bmatrix} - \\ln(ad-bc)= a(x_k^1-\\mu^1)^2 + b(x_k^1-\\mu^1)(x_k^2-\\mu^2) + c(x_k^1-\\mu^1)(x_k^2-\\mu^2) + d(x_k^2-\\mu^2)^2 - \\ln(ad-bc)$\n",
      "\n",
      "$maximize L \\Longrightarrow \\begin{bmatrix} \\frac{\\partial L}{\\partial a} \\\\ \\frac{\\partial L}{\\partial b} \\\\ \\frac{\\partial L}{\\partial c} \\\\ \\frac{\\partial L}{\\partial d} \\end{bmatrix} = 0$\n",
      "\n",
      "${\\displaystyle \\frac{\\partial L}{\\partial a} = \\sum_{k=1}^n [(x_k^1-\\mu^1)^2 - \\frac{d}{ad-bc} ] = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\Longrightarrow \\frac{d}{ad-bc} = \\frac{\\displaystyle \\sum_{k=1}^n (x_k^1-\\mu^1)^2}{n} }$\n",
      "\n",
      "${\\displaystyle \\frac{\\partial L}{\\partial b} = \\sum_{k=1}^n [(x_k^1-\\mu^1)(x_k^2-\\mu^2) + \\frac{c}{ad-bc} ] = 0 \\ \\ \\ \\Longrightarrow \\frac{c}{ad-bc} = \\frac{\\displaystyle \\sum_{k=1}^n (x_k^1-\\mu^1)(x_k^2-\\mu^2)}{n} }$\n",
      "\n",
      "${\\displaystyle \\frac{\\partial L}{\\partial c} = \\sum_{k=1}^n [(x_k^1-\\mu^1)(x_k^2-\\mu^2) + \\frac{b}{ad-bc} ] = 0 \\ \\ \\ \\Longrightarrow \\frac{b}{ad-bc} = \\frac{\\displaystyle \\sum_{k=1}^n (x_k^1-\\mu^1)(x_k^2-\\mu^2)}{n}}$\n",
      "\n",
      "${\\displaystyle \\frac{\\partial L}{\\partial d} = \\sum_{k=1}^n [(x_k^2-\\mu^2)^2 - \\frac{a}{ad-bc} ] = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\Longrightarrow \\frac{a}{ad-bc} = \\frac{\\displaystyle \\sum_{k=1}^n (x_k^2-\\mu^2)^2}{n}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, the actual $\\Sigma$ can be obtained via\n",
      "\n",
      "$\\Sigma = [\\Sigma^{-1}]^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix} = \\begin{bmatrix} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_{22} \\end{bmatrix}$\n",
      "\n",
      "${\\displaystyle \\sigma_{11} = \\frac{d}{ad-bc} = \\frac{\\displaystyle \\sum_{k=1}^n (x_k^1-\\mu^1)^2}{n}  }$\n",
      "\n",
      "${\\displaystyle \\sigma_{12} = \\sigma_{21} = \\frac{-b}{ad-bc} = \\frac{\\displaystyle \\sum_{k=1}^n (x_k^1-\\mu^1)(x_k^2-\\mu^2)}{n}}$\n",
      "\n",
      "${\\displaystyle \\sigma_{22} = \\frac{a}{ad-bc} = \\frac{\\displaystyle \\sum_{k=1}^n (x_k^2-\\mu^2)^2}{n}  }$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}